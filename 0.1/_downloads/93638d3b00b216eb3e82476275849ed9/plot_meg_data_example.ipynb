{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Support recovery on MEG data\n\nThis example compares several methods that recover the support in the MEG/EEG\nsource localization problem with statistical guarantees. Here we work\nwith two datasets that study three different tasks (visual, audio, somato).\n\nWe reproduce the real data experiment of Chevalier et al. (2020) [1]_,\nwhich shows the benefit of (ensemble) clustered inference such as\n(ensemble of) clustered desparsified Multi-Task Lasso ((e)cd-MTLasso)\nover standard approach such as sLORETA. Specifically, it retrieves\nthe support using a natural threshold (not computed a posteriori)\nof the estimated parameter. The estimated support enjoys statistical\nguarantees.\n\n## References\n.. [1] Chevalier, J. A., Gramfort, A., Salmon, J., & Thirion, B. (2020).\n       Statistical control for spatio-temporal MEG/EEG source imaging with\n       desparsified multi-task Lasso. In NeurIPS 2020-34h Conference on\n       Neural Information Processing Systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport mne\nimport numpy as np\nfrom mne.datasets import sample, somato\nfrom mne.inverse_sparse.mxne_inverse import _make_sparse_stc, _prepare_gain\nfrom mne.minimum_norm import apply_inverse, make_inverse_operator\nfrom scipy.sparse.csgraph import connected_components\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.metrics.pairwise import pairwise_distances\n\nfrom hidimstat.clustered_inference import clustered_inference\nfrom hidimstat.ensemble_clustered_inference import ensemble_clustered_inference\nfrom hidimstat.stat_tools import zscore_from_pval\n\noffscreen = True\nsave_fig = False\nplot_saved_fig = False\ninteractive_plot = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Specific preprocessing functions\nThe functions below are used to load or preprocess the data or to put\nthe solution in a convenient format. If you are reading this example\nfor the first time, you should skip this section.\n\nThe following function loads the data from the sample dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _load_sample(cond):\n    \"\"\"Load data from the sample dataset\"\"\"\n\n    # Get data paths\n    subject = \"sample\"\n    data_path = sample.data_path()\n    fwd_fname_suffix = \"MEG/sample/sample_audvis-meg-eeg-oct-6-fwd.fif\"\n    fwd_fname = os.path.join(data_path, fwd_fname_suffix)\n    ave_fname = os.path.join(data_path, \"MEG/sample/sample_audvis-ave.fif\")\n    cov_fname_suffix = \"MEG/sample/sample_audvis-shrunk-cov.fif\"\n    cov_fname = os.path.join(data_path, cov_fname_suffix)\n    cov_fname = data_path + \"/\" + cov_fname_suffix\n    subjects_dir = os.path.join(data_path, \"subjects\")\n\n    if cond == \"audio\":\n        condition = \"Left Auditory\"\n    elif cond == \"visual\":\n        condition = \"Left visual\"\n\n    # Read noise covariance matrix\n    noise_cov = mne.read_cov(cov_fname)\n\n    # Read forward matrix\n    forward = mne.read_forward_solution(fwd_fname)\n\n    # Handling average file\n    evoked = mne.read_evokeds(ave_fname, condition=condition, baseline=(None, 0))\n    evoked = evoked.pick_types(\"grad\")\n\n    # Selecting relevant time window\n    evoked.plot()\n    t_min, t_max = 0.05, 0.1\n    t_step = 0.01\n\n    pca = False\n\n    return (\n        subject,\n        subjects_dir,\n        noise_cov,\n        forward,\n        evoked,\n        t_min,\n        t_max,\n        t_step,\n        pca,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function loads the data from the somato dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _load_somato(cond):\n    \"\"\"Load data from the somato dataset\"\"\"\n\n    # Get data paths\n    data_path = somato.data_path()\n    subject = \"01\"\n    subjects_dir = str(data_path) + \"/derivatives/freesurfer/subjects\"\n    raw_fname = os.path.join(\n        data_path, f\"sub-{subject}\", \"meg\", f\"sub-{subject}_task-{cond}_meg.fif\"\n    )\n    fwd_fname = os.path.join(\n        data_path, \"derivatives\", f\"sub-{subject}\", f\"sub-{subject}_task-{cond}-fwd.fif\"\n    )\n\n    # Read evoked\n    raw = mne.io.read_raw_fif(raw_fname)\n    events = mne.find_events(raw, stim_channel=\"STI 014\")\n    reject = dict(grad=4000e-13, eog=350e-6)\n    picks = mne.pick_types(raw.info, meg=True, eeg=True, eog=True)\n\n    event_id, tmin, tmax = 1, -0.2, 0.25\n    epochs = mne.Epochs(\n        raw, events, event_id, tmin, tmax, picks=picks, reject=reject, preload=True\n    )\n    evoked = epochs.average()\n    evoked = evoked.pick_types(\"grad\")\n    # evoked.plot()\n\n    # Compute noise covariance matrix\n    noise_cov = mne.compute_covariance(epochs, rank=\"info\", tmax=0.0)\n\n    # Read forward matrix\n    forward = mne.read_forward_solution(fwd_fname)\n\n    # Selecting relevant time window: focusing on early signal\n    t_min, t_max = 0.03, 0.05\n    t_step = 1.0 / 300\n\n    # We must reduce the whitener since data were preprocessed for removal\n    # of environmental noise with maxwell filter leading to an effective\n    # number of 64 samples.\n    pca = True\n\n    return (\n        subject,\n        subjects_dir,\n        noise_cov,\n        forward,\n        evoked,\n        t_min,\n        t_max,\n        t_step,\n        pca,\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function below preprocess the raw M/EEG data, it notably computes the\nwhitened MEG/EEG measurements and prepares the gain matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def preprocess_meg_eeg_data(\n    evoked, forward, noise_cov, loose=0.0, depth=0.0, pca=False\n):\n    \"\"\"Preprocess MEG or EEG data to produce the whitened MEG/EEG measurements\n    (target) and the preprocessed gain matrix (design matrix). This function\n    is mainly wrapping the `_prepare_gain` MNE function.\n\n    Parameters\n    ----------\n    evoked : instance of mne.Evoked\n        The evoked data.\n\n    forward : instance of Forward\n        The forward solution.\n\n    noise_cov : instance of Covariance\n        The noise covariance.\n\n    loose : float in [0, 1] or 'auto'\n        Value that weights the source variances of the dipole components\n        that are parallel (tangential) to the cortical surface. If loose\n        is 0 then the solution is computed with fixed orientation.\n        If loose is 1, it corresponds to free orientations.\n        The default value ('auto') is set to 0.2 for surface-oriented source\n        space and set to 1.0 for volumic or discrete source space.\n        See for details:\n        https://mne.tools/stable/auto_tutorials/inverse/35_dipole_orientations.html?highlight=loose\n\n    depth : None or float in [0, 1]\n        Depth weighting coefficients. If None, no depth weighting is performed.\n\n    pca : bool, optional (default=False)\n        If True, whitener is reduced.\n        If False, whitener is not reduced (square matrix).\n\n    Returns\n    -------\n    G : array, shape (n_channels, n_dipoles)\n        The preprocessed gain matrix. If pca=True then n_channels is\n        effectively equal to the rank of the data.\n\n    M : array, shape (n_channels, n_times)\n        The whitened MEG/EEG measurements. If pca=True then n_channels is\n        effectively equal to the rank of the data.\n\n    forward : instance of Forward\n        The preprocessed forward solution.\n    \"\"\"\n\n    all_ch_names = evoked.ch_names\n\n    # Handle depth weighting and whitening (here is no weights)\n    forward, G, gain_info, whitener, _, _ = _prepare_gain(\n        forward,\n        evoked.info,\n        noise_cov,\n        pca=pca,\n        depth=depth,\n        loose=loose,\n        weights=None,\n        weights_min=None,\n        rank=None,\n    )\n\n    # Select channels of interest\n    sel = [all_ch_names.index(name) for name in gain_info[\"ch_names\"]]\n\n    M = evoked.data[sel]\n    M = np.dot(whitener, M)\n\n    return G, M, forward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The next function translates the solution in a readable format for the\nMNE plotting functions that require a Source Time Course (STC) object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _compute_stc(zscore_active_set, active_set, evoked, forward):\n    \"\"\"Wrapper of `_make_sparse_stc`\"\"\"\n\n    X = np.atleast_2d(zscore_active_set)\n\n    if X.shape[1] > 1 and X.shape[0] == 1:\n        X = X.T\n\n    stc = _make_sparse_stc(\n        X, active_set, forward, tmin=evoked.times[0], tstep=1.0 / evoked.info[\"sfreq\"]\n    )\n    return stc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The function below will be used to modify the connectivity matrix\nto avoid multiple warnings when we run the clustering algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def _fix_connectivity(X, connectivity, affinity):\n    \"\"\"Complete the connectivity matrix if necessary\"\"\"\n\n    # Convert connectivity matrix into LIL format\n    connectivity = connectivity.tolil()\n\n    # Compute the number of nodes\n    n_connected_components, labels = connected_components(connectivity)\n\n    if n_connected_components > 1:\n\n        for i in range(n_connected_components):\n            idx_i = np.where(labels == i)[0]\n            Xi = X[idx_i]\n            for j in range(i):\n                idx_j = np.where(labels == j)[0]\n                Xj = X[idx_j]\n                D = pairwise_distances(Xi, Xj, metric=affinity)\n                ii, jj = np.where(D == np.min(D))\n                ii = ii[0]\n                jj = jj[0]\n                connectivity[idx_i[ii], idx_j[jj]] = True\n                connectivity[idx_j[jj], idx_i[ii]] = True\n\n    return connectivity, n_connected_components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading data\n\nAfter choosing a task, we run the function that loads the data to get\nthe corresponding evoked, forward and noise covariance matrices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Choose the experiment (task)\nlist_cond = [\"audio\", \"visual\", \"somato\"]\ncond = list_cond[2]\nprint(f\"Let's process the condition: {cond}\")\n\n# Load the data\nif cond in [\"audio\", \"visual\"]:\n    sub, subs_dir, noise_cov, forward, evoked, t_min, t_max, t_step, pca = _load_sample(\n        cond\n    )\n\nelif cond == \"somato\":\n    sub, subs_dir, noise_cov, forward, evoked, t_min, t_max, t_step, pca = _load_somato(\n        cond\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparing data for clustered inference\n\nFor clustered inference we need the targets ``Y``, the design matrix ``X``\nand the ``connectivity`` matrix, which is a sparse adjacency matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Collecting features' connectivity\nconnectivity = mne.source_estimate.spatial_src_adjacency(forward[\"src\"])\n\n# Croping evoked according to relevant time window\nevoked.crop(tmin=t_min, tmax=t_max)\n\n# Choosing frequency and number of clusters used for compression.\n# Reducing the frequency to 100Hz to make inference faster\nstep = int(t_step * evoked.info[\"sfreq\"])\nevoked.decimate(step)\nt_min = evoked.times[0]\nt_step = 1.0 / evoked.info[\"sfreq\"]\n\n# Preprocessing MEG data\nX, Y, forward = preprocess_meg_eeg_data(evoked, forward, noise_cov, pca=pca)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running clustered inference\n\nFor MEG data ``n_clusters = 1000`` is generally a good default choice.\nTaking ``n_clusters > 2000`` might lead to an unpowerful inference.\nTaking ``n_clusters < 500`` might compress too much the data leading\nto a compressed problem not close enough to the original problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_clusters = 1000\n\n# Setting theoretical FWER target\nfwer_target = 0.1\n\n# Computing threshold taking into account for Bonferroni correction\ncorrection_clust_inf = 1.0 / n_clusters\nzscore_threshold = zscore_from_pval((fwer_target / 2) * correction_clust_inf)\n\n# Initializing FeatureAgglomeration object used for the clustering step\nconnectivity_fixed, _ = _fix_connectivity(X.T, connectivity, affinity=\"euclidean\")\nward = FeatureAgglomeration(n_clusters=n_clusters, connectivity=connectivity)\n\n# Making the inference with the clustered inference algorithm\ninference_method = \"desparsified-group-lasso\"\nbeta_hat, pval, pval_corr, one_minus_pval, one_minus_pval_corr = clustered_inference(\n    X, Y, ward, n_clusters, method=inference_method\n)\n\n# Extracting active set (support)\nactive_set = np.logical_or(\n    pval_corr < fwer_target / 2, one_minus_pval_corr < fwer_target / 2\n)\nactive_set_full = np.copy(active_set)\nactive_set_full[:] = True\n\n# Translating p-vals into z-scores for nicer visualization\nzscore = zscore_from_pval(pval, one_minus_pval)\nzscore_active_set = zscore[active_set]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization\nNow, let us plot the thresholded statistical maps derived thanks to the\nclustered inference algorithm referred as cd-MTLasso.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's put the solution into the format supported by the plotting functions\nstc = _compute_stc(zscore_active_set, active_set, evoked, forward)\n\n# Plotting parameters\nif cond == \"audio\":\n    hemi = \"lh\"\n    view = \"lat\"\nelif cond == \"visual\":\n    hemi = \"rh\"\n    view = \"med\"\nelif cond == \"somato\":\n    hemi = \"rh\"\n    view = \"lat\"\n\n\nif active_set.sum() != 0:\n    max_stc = np.max(np.abs(stc.data))\n    clim = dict(pos_lims=(3, zscore_threshold, max_stc), kind=\"value\")\n    brain = stc.plot(\n        subject=sub,\n        hemi=hemi,\n        clim=clim,\n        subjects_dir=subs_dir,\n        views=view,\n        time_viewer=False,\n        backend=\"matplotlib\" if offscreen else \"pyvistaqt\",\n    )\n    if offscreen:\n        brain.text(0.05, 0.9, f\"{cond} - cd-MTLasso\", fontsize=20)\n    else:\n        brain.add_text(0.05, 0.9, f\"{cond} - cd-MTLasso\", \"title\", font_size=20)\n\n\n# Hack for nice figures on HiDimStat website\nif save_fig:\n    brain.save_image(f\"figures/meg_{cond}_cd-MTLasso.png\")\nif plot_saved_fig:\n    brain.close()\n    img = mpimg.imread(f\"figures/meg_{cond}_cd-MTLasso.png\")\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.show()\n\nif interactive_plot:\n    brain = stc.plot(subject=sub, hemi=\"both\", subjects_dir=subs_dir, clim=clim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with sLORETA\nNow, we compare the results derived from cd-MTLasso with the solution\nobtained from the one of the most standard approach: sLORETA.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Running sLORETA with standard hyper-parameter\nlambda2 = 1.0 / 9\ninv = make_inverse_operator(\n    evoked.info, forward, noise_cov, loose=0.0, depth=0.0, fixed=True\n)\nstc_full = apply_inverse(evoked, inv, lambda2=lambda2, method=\"sLORETA\")\nstc_full = stc_full.mean()\n\n# Computing threshold taking into account for Bonferroni correction\nn_features = stc_full.data.size\ncorrection = 1.0 / n_features\nzscore_threshold_no_clust = zscore_from_pval((fwer_target / 2) * correction)\n\n# Computing estimated support by sLORETA\nactive_set = np.abs(stc_full.data) > zscore_threshold_no_clust\nactive_set = active_set.flatten()\n\n# Putting the solution into the format supported by the plotting functions\nsLORETA_solution = np.atleast_2d(stc_full.data[active_set]).flatten()\nstc = _make_sparse_stc(\n    sLORETA_solution, active_set, forward, stc_full.tmin, tstep=stc_full.tstep\n)\n\n# Plotting sLORETA solution\nif active_set.sum() != 0:\n    max_stc = np.max(np.abs(stc.data))\n    clim = dict(pos_lims=(3, zscore_threshold_no_clust, max_stc), kind=\"value\")\n    brain = stc.plot(\n        subject=sub,\n        hemi=hemi,\n        clim=clim,\n        subjects_dir=subs_dir,\n        views=view,\n        time_viewer=False,\n        backend=\"matplotlib\" if offscreen else \"pyvistaqt\",\n    )\n    if offscreen:\n        brain.text(0.05, 0.9, f\"{cond} - sLORETA\", fontsize=20)\n    else:\n        brain.add_text(0.05, 0.9, f\"{cond} - sLORETA\", \"title\", font_size=20)\n\n    # Hack for nice figures on HiDimStat website\n    if save_fig:\n        brain.save_image(f\"figures/meg_{cond}_sLORETA.png\")\n    if plot_saved_fig:\n        brain.close()\n        img = mpimg.imread(f\"figures/meg_{cond}_sLORETA.png\")\n        plt.imshow(img)\n        plt.axis(\"off\")\n        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis of the results\nWhile the clustered inference solution always highlights the expected\ncortex (audio, visual or somato-sensory) with a universal predertemined\nthreshold, the solution derived from the sLORETA method does not enjoy\nthe same property. For the audio task the method is conservative and\nfor the somato task the method makes false discoveries (then it seems\nanti-conservative).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running ensemble clustered inference\n\nTo go further it is possible to run the ensemble clustered inference\nalgorithm. It might take several minutes on standard device with\n``n_jobs=1`` (around 10 min). Just set\n``run_ensemble_clustered_inference=True`` below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "run_ensemble_clustered_inference = False\n\nif run_ensemble_clustered_inference:\n    # Making the inference with the ensembled clustered inference algorithm\n    beta_hat, pval, pval_corr, one_minus_pval, one_minus_pval_corr = (\n        ensemble_clustered_inference(\n            X, Y, ward, n_clusters, inference_method=inference_method\n        )\n    )\n\n    # Extracting active set (support)\n    active_set = np.logical_or(\n        pval_corr < fwer_target / 2, one_minus_pval_corr < fwer_target / 2\n    )\n    active_set_full = np.copy(active_set)\n    active_set_full[:] = True\n\n    # Translating p-vals into z-scores for nicer visualization\n    zscore = zscore_from_pval(pval, one_minus_pval)\n    zscore_active_set = zscore[active_set]\n\n    # Putting the solution into the format supported by the plotting functions\n    stc = _compute_stc(zscore_active_set, active_set, evoked, forward)\n\n    # Plotting ensemble clustered inference solution\n    if active_set.sum() != 0:\n        max_stc = np.max(np.abs(stc._data))\n        clim = dict(pos_lims=(3, zscore_threshold, max_stc), kind=\"value\")\n        brain = stc.plot(\n            subject=sub,\n            hemi=hemi,\n            clim=clim,\n            subjects_dir=subs_dir,\n            views=view,\n            time_viewer=False,\n            backend=\"matplotlib\" if offscreen else \"pyvistaqt\",\n        )\n        if offscreen:\n            brain.text(0.05, 0.9, f\"{cond} - ecd-MTLasso\", fontsize=20)\n        else:\n            brain.add_text(0.05, 0.9, f\"{cond} - ecd-MTLasso\", \"title\", font_size=20)\n\n        # Hack for nice figures on HiDimStat website\n        if save_fig:\n            brain.save_image(f\"figures/meg_{cond}_ecd-MTLasso.png\")\n        if plot_saved_fig:\n            brain.close()\n            img = mpimg.imread(f\"figures/meg_{cond}_ecd-MTLasso.png\")\n            plt.imshow(img)\n            plt.axis(\"off\")\n            plt.show()\n\n        if interactive_plot:\n            brain = stc.plot(subject=sub, hemi=\"both\", subjects_dir=subs_dir, clim=clim)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}