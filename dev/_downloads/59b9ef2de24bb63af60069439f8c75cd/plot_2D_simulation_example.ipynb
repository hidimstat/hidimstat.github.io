{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Ensemble Clustered Inference on 2D Data\n\nIn this example, we present how to perform inference on simulated 2D data. This setting\nis particularly challenging since the number of features (pixels in 2D) is much larger\nthan the number of samples. We first illustrate the limitations of the Desparsified\nLasso method in this setting and then present two methods that leverage the data's\nspatial structure to build clusters and perform the inference at the cluster level.\nWe first show how to use clustered inference with DL (:class:`hidimstat.CluDL`).\nWe then show how to use clustered inference with Ensembled CluDL\n(:class:`hidimstat.EnCluDL`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating the data\nWe begin by simulating 2D data where the support (i.e., the predictive features)\nconsists of four regions of neighboring pixels located in each corner of the 2D image.\nThe target variable $y$ is a continuous variable generated from a linear model. To\nmake the problem more challenging, the pixels are spatially correlated using a\nGaussian filter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import ListedColormap\nfrom matplotlib.patches import Patch\n\nfrom hidimstat._utils.scenario import multivariate_simulation_spatial\n\nn_samples = 100\nshape = (40, 40)\nn_features = shape[1] * shape[0]\nroi_size = 5  # size of the edge of the four predictive regions\nsignal_noise_ratio = 16.0  # noise standard deviation\nsmooth_X = 1.0  # level of spatial smoothing introduced by the Gaussian filter\n\n# generating the data\nX_init, y, beta, epsilon = multivariate_simulation_spatial(\n    n_samples, shape, roi_size, signal_noise_ratio, smooth_X, seed=0\n)\nprint(f\"Number of samples: {X_init.shape[0]}, Number of features: {n_features}\")\n\n\n# visualize the data\ncmap = ListedColormap([\"white\", \"tab:green\"])  # 0 -> blue (null), 1 -> green (support)\nfig, ax = plt.subplots(figsize=(4, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\nax.imshow(\n    beta.reshape(shape),\n    cmap=cmap,\n    vmin=0,\n    vmax=1,\n)\n\n# Legend: green = support, blue = null\nlegend_handles = [\n    Patch(facecolor=\"tab:green\", edgecolor=\"k\", label=\"Support\"),\n    Patch(facecolor=\"white\", edgecolor=\"k\", label=\"Null\"),\n]\nax.legend(handles=legend_handles, loc=\"lower center\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with Desparsified Lasso\nFirst, we perform inference using the Desparsified Lasso method, treating the data as\na standard high-dimensional regression problem without considering its spatial\nstructure. The aim of the inference step is to recover the support while controlling\nthe Family-Wise Error Rate (FWER) at a targeted level of 0.1. To achieve this, we use\nthe Bonferroni correction, applying a factor equal to the number of features. For more\ndetails about the Desparsified Lasso method, see :footcite:t:`javanmard2014confidence`,\n:footcite:t:`zhang2014confidence` and :footcite:t:`van2014asymptotically`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.linear_model import LassoCV\n\nfrom hidimstat import DesparsifiedLasso\n\nfwer_target = 0.1\nn_jobs = 5  # number of workers for parallel computing\n\n# compute desparsified lasso\nestimator = LassoCV(max_iter=1000, tol=0.0001, eps=0.01, fit_intercept=False)\ndl = DesparsifiedLasso(estimator=estimator, n_jobs=n_jobs, random_state=0)\ndl.fit_importance(X_init, y)\n\n# compute estimated support (first method)\nselected_dl = (dl.pvalues_ < (fwer_target / 2) / n_features) | (\n    dl.one_minus_pvalues_ < (fwer_target / 2) / n_features\n)\n\ntp_mask = ((selected_dl.astype(int) == 1) & (beta == 1)).astype(bool)\nfp_mask = ((selected_dl.astype(int) == 1) & (beta == 0)).astype(bool)\nfn_mask = ((selected_dl.astype(int) == 0) & (beta == 1)).astype(bool)\nmask_dl = np.zeros(shape)\nmask_dl[tp_mask.reshape(shape)] = 1\nmask_dl[fp_mask.reshape(shape)] = -2\nmask_dl[fn_mask.reshape(shape)] = -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We visualize the support estimated by the Desparsified Lasso method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, ax = plt.subplots(figsize=(4, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\ncmap = ListedColormap([\"tab:red\", \"tab:purple\", \"white\", \"tab:green\"])\nax.imshow(mask_dl, cmap=cmap, vmin=-2, vmax=1)\nlegend_handles = [\n    Patch(facecolor=\"tab:green\", edgecolor=\"k\", label=\"True Positive\"),\n    Patch(facecolor=\"tab:red\", edgecolor=\"k\", label=\"False Positive\"),\n    Patch(facecolor=\"tab:purple\", edgecolor=\"k\", label=\"False Negative\"),\n]\nax.legend(handles=legend_handles, loc=\"lower center\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It can be seen that:\n**1.** the Desparsified Lasso method is not powerful enough to recover the support,\nit only selects scattered pixels as true positives without identifying the regions.\n**2.** the number of false positives is quite high, and sometimes false positives are\nlocated far from the true support.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustered inference with CluDL\nTo improve the power of the inference, we can leverage the spatial structure of the\ndata. The idea is to group correlated pixels into clusters, using an additional\nspatial constraint (pixels are iteratively merged with neighboring pixels). This\napproach leads to a dimension reduction while preserving the data's spatial structure.\nTo control the FWER at the targeted level of 0.1, we perform Bonferroni correction,\nbut here the correction factor is equal to the number of clusters instead of the\nnumber of features. For more details about CluDL, see\n:footcite:t:`chevalier2022spatially`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\nfrom sklearn.cluster import FeatureAgglomeration\nfrom sklearn.feature_extraction import image\n\nfrom hidimstat import CluDL\n\n# Clustering step\nn_clusters = 200\nconnectivity = image.grid_to_graph(n_x=shape[0], n_y=shape[1])\nclustering = FeatureAgglomeration(\n    n_clusters=n_clusters, connectivity=connectivity, linkage=\"ward\"\n)\n\ndl_2 = DesparsifiedLasso(estimator=clone(estimator), n_jobs=n_jobs)\nclu_dl = CluDL(desparsified_lasso=dl_2, clustering=clustering, random_state=0)\nclu_dl.fit_importance(X_init, y)\n\nselected_cdl = (clu_dl.pvalues_ < (fwer_target / 2) / n_clusters) | (\n    clu_dl.one_minus_pvalues_ < (fwer_target / 2) / n_clusters\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the support estimated by CluDL\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tp_mask = ((selected_cdl.astype(int) == 1) & (beta == 1)).astype(bool)\nfp_mask = ((selected_cdl.astype(int) == 1) & (beta == 0)).astype(bool)\nfn_mask = ((selected_cdl.astype(int) == 0) & (beta == 1)).astype(bool)\nmask_cludl = np.zeros(shape)\nmask_cludl[tp_mask.reshape(shape)] = 1\nmask_cludl[fp_mask.reshape(shape)] = -2\nmask_cludl[fn_mask.reshape(shape)] = -1\n\n_, ax = plt.subplots(figsize=(4, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\ncmap = ListedColormap([\"tab:red\", \"tab:purple\", \"white\", \"tab:green\"])\nax.imshow(mask_cludl, cmap=cmap, vmin=-2, vmax=1)\nlegend_handles = [\n    Patch(facecolor=\"tab:green\", edgecolor=\"k\", label=\"True Positive\"),\n    Patch(facecolor=\"tab:red\", edgecolor=\"k\", label=\"False Positive\"),\n    Patch(facecolor=\"tab:purple\", edgecolor=\"k\", label=\"False Negative\"),\n]\nax.legend(handles=legend_handles, loc=\"lower center\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With ``CluDL``, the support recovery is more powerful, with large regions of the true\nsupport being correctly identified. However, some false positives remain. In this\ncase, these false positives consist of small clusters that can be either contiguous to\nthe true support or located far from it.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with Ensembled CluDL\nFinally, we perform inference using an ensembled version of ``CluDL`` called ``EnCluDL``. The\nidea is to run several ``CluDL`` algorithms with different clustering choices. By\nrepeating the clustering step on different bootstrap samples of the data, this\napproach derandomizes the clustering choice and makes it more robust to small\nvariations in the data. It can be efficiently parallelized since the different ``CluDL``\nruns are independent and thus embarrassingly parallel. Similar to ``CluDL``, we perform\nBonferroni correction with a factor equal to the number of clusters on the p-values\nobtained by aggregating the different ``CluDL`` runs. For more details about ``EnCluDL``,\nsee :footcite:t:`chevalier2022spatially`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat.ensemble_clustered_inference import EnCluDL\n\nenclu_dl = EnCluDL(\n    desparsified_lasso=DesparsifiedLasso(estimator=clone(estimator), n_jobs=1),\n    clustering=clustering,\n    n_bootstraps=20,\n    random_state=0,\n    n_jobs=n_jobs,\n    cluster_boostrap_size=0.5,\n)\nenclu_dl.fit_importance(X_init, y)\n\nselected_ecdl = (enclu_dl.pvalues_ < (fwer_target / 2) / n_clusters) | (\n    enclu_dl.one_minus_pvalues_ < (fwer_target / 2) / n_clusters\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing the support estimated by EnCluDL\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tp_mask = ((selected_ecdl.astype(int) == 1) & (beta == 1)).astype(bool)\nfp_mask = ((selected_ecdl.astype(int) == 1) & (beta == 0)).astype(bool)\nfn_mask = ((selected_ecdl.astype(int) == 0) & (beta == 1)).astype(bool)\nmask_encludl = np.zeros(shape)\nmask_encludl[tp_mask.reshape(shape)] = 1\nmask_encludl[fp_mask.reshape(shape)] = -2\nmask_encludl[fn_mask.reshape(shape)] = -1\n\n\n_, ax = plt.subplots(figsize=(4, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\ncmap = ListedColormap([\"tab:red\", \"tab:purple\", \"white\", \"tab:green\"])\nax.imshow(mask_encludl, cmap=cmap, vmin=-2, vmax=1)\nlegend_handles = [\n    Patch(facecolor=\"tab:green\", edgecolor=\"k\", label=\"True Positive\"),\n    Patch(facecolor=\"tab:red\", edgecolor=\"k\", label=\"False Positive\"),\n    Patch(facecolor=\"tab:purple\", edgecolor=\"k\", label=\"False Negative\"),\n]\nax.legend(handles=legend_handles, loc=\"lower center\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to ``CluDL``, ``EnCluDL`` further improves the inference, primarily by reducing the\nnumber of false positives located far from the true support. An intuitive explanation\nfor this improvement is that while false discoveries far from the true support may\noccur randomly in a single clustering, they are less likely to occur repeatedly in\noverlapping clusters obtained from different bootstrap samples of the data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support recovery with spatial tolerance\nFor spatially structured data, false discoveries that are close to the true support,\nor even part of clusters that intersect the true support, are likely to change less\nthe interpretation of the results than false discoveries located far from the support.\nTherefore, it can be relevant to consider a spatially relaxed support recovery,\nThe idea is to penalize less those false discoveries that are close\nto the true support. To achieve this, we introduce an extended support that includes\nthe true support and a tolerance region around it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "spatial_tolerance = 3\nroi_size_extended = roi_size + spatial_tolerance\nbeta_extended = beta.copy().reshape(shape)\nbeta_extended[0:roi_size_extended, 0:roi_size_extended] += 1\nbeta_extended[-roi_size_extended:, -roi_size_extended:] += 1\nbeta_extended[0:roi_size_extended, -roi_size_extended:] += 1\nbeta_extended[-roi_size_extended:, 0:roi_size_extended] += 1\n\n# visualize the extended support\ncmap = ListedColormap([\"white\", \"tab:orange\", \"tab:green\"])\nfig, ax = plt.subplots(figsize=(4, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\nax.imshow(beta_extended.reshape(shape), cmap=cmap, vmin=0, vmax=2)\n# Legend: green = support, yellow = tolerance region, white = null\nlegend_handles = [\n    Patch(facecolor=\"tab:green\", edgecolor=\"k\", label=\"Support\"),\n    Patch(facecolor=\"tab:orange\", edgecolor=\"k\", label=\"Tolerance region\"),\n    Patch(facecolor=\"white\", edgecolor=\"k\", label=\"Null\"),\n]\nax.legend(handles=legend_handles, loc=\"lower center\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with spatial tolerance\nNow, we compare the three methods presented above (Desparsified Lasso, CluDL and\nEnCluDL) in terms of support recovery with spatial tolerance.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def compute_spatially_relaxed_mask(mask, beta_extended):\n    \"\"\"Mark the false positives that are in the tolerance with a special value, -3\"\"\"\n    support = beta_extended.reshape(shape) >= 1\n    mask[support & (mask == -2)] = -3\n    return mask\n\n\nmask_dl_relaxed = compute_spatially_relaxed_mask(mask_dl, beta_extended)\nmask_cludl_relaxed = compute_spatially_relaxed_mask(mask_cludl, beta_extended)\nmask_encldl_relaxed = compute_spatially_relaxed_mask(mask_encludl, beta_extended)\ncmap = ListedColormap([\"tab:orange\", \"tab:red\", \"tab:purple\", \"white\", \"tab:green\"])\n\n# sphinx_gallery_thumbnail_number = 6\n_, axes = plt.subplots(1, 3, figsize=(10, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\naxes[0].imshow(mask_dl_relaxed, cmap=cmap, vmin=-3, vmax=1)\naxes[0].set_title(\"Desparsified Lasso\", fontweight=\"bold\")\naxes[1].imshow(mask_cludl_relaxed, cmap=cmap, vmin=-3, vmax=1)\naxes[1].set_title(\"CluDL\", fontweight=\"bold\")\naxes[2].imshow(mask_encldl_relaxed, cmap=cmap, vmin=-3, vmax=1)\naxes[2].set_title(\"EnCluDL\", fontweight=\"bold\")\nlegend_handles = [\n    Patch(facecolor=\"tab:green\", edgecolor=\"k\", label=\"True Positive\"),\n    Patch(facecolor=\"tab:orange\", edgecolor=\"k\", label=\"False Positive in tolerance\"),\n    Patch(facecolor=\"tab:red\", edgecolor=\"k\", label=\"False Positive out of tolerance\"),\n    Patch(facecolor=\"tab:purple\", edgecolor=\"k\", label=\"False Negative\"),\n]\naxes[0].legend(handles=legend_handles, loc=\"lower center\")\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Note: Choosing inference parameters\nThe choice of the number of clusters depends on several parameters, such as:\nthe structure of the data (a higher correlation between neighboring features\nenable a greater dimension reduction, i.e. a smaller number of clusters),\nthe number of samples (small datasets require more dimension reduction) and\nthe required spatial tolerance (small clusters lead to limited spatial\nuncertainty). Formally, \"spatial tolerance\" is defined by the largest\ndistance from the true support for which the occurrence of a false discovery\nis not statistically controlled (c.f. :footcite:t:`chevalier2022spatially`).\nTheoretically, the spatial tolerance ``delta`` is equal to the largest\ncluster diameter. However this choice is conservative, notably in the case\nof ensembled clustered inference. For these algorithms, we recommend to take\nthe average cluster radius. In this example, we choose ``n_clusters = 200``,\nleading to a theoretical spatial tolerance ``delta = 6``, which is still\nconservative (see Results).\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}