{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Variable Importance on diabetes dataset\n\nVariable Importance estimates the influence of a given input variable to the\nprediction made by a model. To assess variable importance in a prediction\nproblem, :footcite:t:`breimanRandomForests2001` introduced the permutation\napproach where the values are shuffled for one variable/column at a time. This\npermutation breaks the relationship between the variable of interest and the\noutcome. Following, the loss score is checked before and after this\nsubstitution for any significant drop in the performance which reflects the\nsignificance of this variable to predict the outcome. This ease-to-use solution\nis demonstrated, in the work by\n:footcite:t:`stroblConditionalVariableImportance2008`, to be affected by the\ndegree of correlation between the variables, thus biased towards truly\nnon-significant variables highly correlated with the significant ones and\ncreating fake significant variables. They introduced a solution for the Random\nForest estimator based on conditional sampling by performing sub-groups\npermutation when bisecting the space using the conditioning variables of the\nbuilding process. However, this solution is exclusive to the Random Forest and\nis costly with high-dimensional settings.\n:footcite:t:`Chamma_NeurIPS2023` introduced a new model-agnostic solution to\nbypass the limitations of the permutation approach under the use of the\nconditional schemes. The variable of interest does contain two types of\ninformation: 1) the relationship with the remaining variables and 2) the\nrelationship with the outcome. The standard permutation, while breaking the\nrelationship with the outcome, is also destroying the dependency with the\nremaining variables. Therefore, instead of directly permuting the variable of\ninterest, the variable of interest is predicted by the remaining\nvariables and the residuals of this prediction are permuted before\nreconstructing the new version of the variable. This solution preserves the\ndependency with the remaining variables.\n\nIn this example, we compare both the standard permutation and its conditional\nvariant approaches for variable importance on the diabetes dataset for the\nsingle-level case. The aim is to see if integrating the new\nstatistically-controlled solution has an impact on the results.\n\n## References\n.. footbibliography::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the diabetes dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Encode sex as binary\nX[:, 1] = (X[:, 1] > 0.0).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a baseline model on the diabetes dataset\nWe use a Ridge regression model with a 5-fold cross-validation to fit the\ndiabetes dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.base import clone\nfrom sklearn.linear_model import LogisticRegressionCV, RidgeCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold\n\nn_folds = 5\nregressor = RidgeCV(\n    alphas=np.logspace(-3, 3, 10),\n    cv=KFold(shuffle=True, random_state=20),\n)\nregressor_list = [clone(regressor) for _ in range(n_folds)]\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=21)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    regressor_list[i].fit(X[train_index], y[train_index])\n    score = r2_score(\n        y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index])\n    )\n    mse = mean_squared_error(\n        y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index])\n    )\n\n    print(f\"Fold {i}: {score=}\")\n    print(f\"Fold {i}: {mse=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the CFI method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import CFI\n\ncfi_importance_list = []\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=21)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}\")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    cfi = CFI(\n        estimator=regressor_list[i],\n        imputation_model_continuous=RidgeCV(alphas=np.logspace(-3, 3, 10), cv=KFold()),\n        imputation_model_categorical=LogisticRegressionCV(\n            Cs=np.logspace(-2, 2, 10),\n            cv=KFold(),\n        ),\n        # covariate_estimator=HistGradientBoostingRegressor(random_state=0,),\n        n_permutations=50,\n        random_state=24,\n        n_jobs=4,\n    )\n    cfi.fit(X_train, y_train)\n    importance = cfi.importance(X_test, y_test)\n    cfi_importance_list.append(importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the LOCO method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import LOCO\n\nloco_importance_list = []\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=21)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}\")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    loco = LOCO(\n        estimator=regressor_list[i],\n        n_jobs=4,\n    )\n    loco.fit(X_train, y_train)\n    importance = loco.importance(X_test, y_test)\n    loco_importance_list.append(importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables using the permutation method\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import PFI\n\npfi_importance_list = []\nkf = KFold(n_splits=n_folds, shuffle=True, random_state=21)\nfor i, (train_index, test_index) in enumerate(kf.split(X)):\n    print(f\"Fold {i}\")\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    pfi = PFI(\n        estimator=regressor_list[i],\n        n_permutations=50,\n        random_state=25,\n        n_jobs=4,\n    )\n    pfi.fit(X_train, y_train)\n    importance = pfi.importance(X_test, y_test)\n    pfi_importance_list.append(importance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze the results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\nfrom scipy.stats import ttest_1samp\n\ncfi_vim_arr = np.array(cfi_importance_list) / 2\ncfi_pval = ttest_1samp(cfi_vim_arr, 0, alternative=\"greater\").pvalue\n\nvim = [\n    pd.DataFrame(\n        {\n            \"var\": np.arange(cfi_vim_arr.shape[1]),\n            \"importance\": x,\n            \"fold\": i,\n            \"pval\": cfi_pval,\n            \"method\": \"CFI\",\n        }\n    )\n    for x in cfi_importance_list\n]\n\nloco_vim_arr = np.array(loco_importance_list)\nloco_pval = ttest_1samp(loco_vim_arr, 0, alternative=\"greater\").pvalue\n\nvim += [\n    pd.DataFrame(\n        {\n            \"var\": np.arange(loco_vim_arr.shape[1]),\n            \"importance\": x,\n            \"fold\": i,\n            \"pval\": loco_pval,\n            \"method\": \"LOCO\",\n        }\n    )\n    for x in loco_importance_list\n]\n\npfi_vim_arr = np.array(pfi_importance_list)\npfi_pval = ttest_1samp(pfi_vim_arr, 0, alternative=\"greater\").pvalue\n\nvim += [\n    pd.DataFrame(\n        {\n            \"var\": np.arange(pfi_vim_arr.shape[1]),\n            \"importance\": x,\n            \"fold\": i,\n            \"pval\": pfi_pval,\n            \"method\": \"PFI\",\n        }\n    )\n    for x in pfi_importance_list\n]\n\nfig, ax = plt.subplots()\ndf_plot = pd.concat(vim)\ndf_plot[\"pval\"] = -np.log10(df_plot[\"pval\"])\nmethods = df_plot[\"method\"].unique()\ncolors = plt.get_cmap(\"tab10\", 10)\n\nfor i, method in enumerate(methods):\n    subset = df_plot[df_plot[\"method\"] == method]\n    ax.bar(\n        subset[\"var\"] + i * 0.2,\n        subset[\"pval\"],\n        width=0.2,\n        label=method,\n        color=colors(i),\n    )\n\nax.legend(title=\"Method\")\nax.set_ylabel(r\"$-\\log_{10}(\\text{p-value})$\")\nax.axhline(-np.log10(0.05), color=\"tab:red\", ls=\"--\")\nax.set_xlabel(\"Variable\")\nax.set_xticks(range(len(diabetes.feature_names)))\nax.set_xticklabels(diabetes.feature_names)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}