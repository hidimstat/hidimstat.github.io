{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Visualization with Partial Dependency Plots\n\nThis example demonstrates how to create Partial Dependency Plots (PDPs). This\nvisualization method allows you to examine a model's dependence on a single feature or\na pair of features. The underlying implementation is built upon\nsklearn.inspection.partial_dependence, which calculates the dependence by taking the\naverage response of an estimator across all possible values of the target feature(s).\nWe'll use the circles dataset to illustrate the basic usage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the circles dataset\nWe start by sampling a synthetic dataset using the `make_circles` function from\n`sklearn.datasets`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import make_circles\n\nX, y = make_circles(n_samples=500, noise=0.1, factor=0.7, random_state=0)\n\n# Visualizing the dataset\n_, ax = plt.subplots()\nsns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y, ax=ax)\nax.set_xlabel(\"X0\")\nax.set_ylabel(\"X1\")\nsns.despine(ax=ax)\nc1 = plt.Circle((0, 0), 0.85, color=\"k\", ls=\"--\", fill=False, label=\"class boundary\")\nax.add_patch(c1)\n_ = ax.legend(loc=\"upper right\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training a classifier\nNext, we train a model to solve the binary classification task presented by the\nnon-linearly separable circles dataset. For this example, we'll use a gradient\nboosted tree ensemble, specifically the HistGradientBoostingClassifier from\nscikit-learn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nmodel = HistGradientBoostingClassifier(random_state=0)\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict_proba(X_test)\n\nauc = roc_auc_score(y_true=y_test, y_score=y_pred[:, 1])\nprint(f\"ROC AUC on the test set: {auc:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partial Dependence for an Individual Feature\nOnce the model is fitted, we use the Partial Dependency Plot (PDP) to visualize its\ndependence on a single input feature (e.g., the first feature, $X_0$).The\nresulting plot shows the average response of the model (on the :math`y`-axis)\nfor each possible value of the selected feature (on the $x$-axis), with the averaging\nperformed over all other features in the dataset.\n\nThe plot also includes the marginal distribution of the feature considered along\nthe $x$-axis. This feature distribution is essential for identifying\nlow-density regions in the data. Model predictions and the estimated partial\ndependence can be less reliable or extrapolated in these regions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat.visualization import PDP\n\n# sphinx_gallery_thumbnail_number = 2\npdp = PDP(model)\n_ = pdp.plot(X_test, features=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Partial Dependence on a Pair of Features\nWe can similarly visualize the dependence of the model on a pair of features\n(e.g., $X_0$ and $X_1$). Here, the partial dependence is encoded by\ncontour lines (level lines) across the 2D plot. The marginal distribution for each\nfeature is also represented along the axes to help identify regions where the\nestimated dependence might be unreliable due to a low density of training data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "axes = pdp.plot(X_test, features=[0, 1], cmap=\"RdBu_r\")\nc1 = plt.Circle((0, 0), 0.85, color=\"k\", ls=\"--\", fill=False, zorder=10)\n_ = axes[1, 0].add_patch(c1)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}