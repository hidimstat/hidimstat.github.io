{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Conditional Feature Importance (CFI) on the wine dataset\n\nThis example demonstrates how to measure feature importance using CFI [:footcite:t:`Chamma_NeurIPS2023`] on the wine dataset.\nThe data are the results of chemical analyses of wines grown in the same region in Italy,\nderived from three different cultivars. Thirteen features are used to predict three types\nof wine, making this a 3-class classification problem. In this example, we show how to\nuse CFI to identify which variables are most important for solving the classification\ntask with a neural network classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and preparing the data\nWe start by loading the dataset and splitting it into training and test sets.\nThis split will be used both for training the classifier and for the CFI method.\nThe CFI method measures the importance of a feature by generating perturbations\nthrough sampling from the conditional distribution $p(X^j | X^{-j})$,\nwhich is estimated on the training set.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\nX, y = load_wine(return_X_y=True)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X,\n    y,\n    test_size=0.5,\n    random_state=0,\n    stratify=y,\n    shuffle=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fitting the model and computing CFI feature importance\nTo solve the classification task, we use a pipeline that first standardizes the features with StandardScaler,\nfollowed by a neural network (MLPClassifier) with one hidden layer of 100 neurons.\nBefore measuring feature importance, we evaluate the estimator's performance by reporting its accuracy score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nclf = make_pipeline(\n    StandardScaler(),\n    MLPClassifier(\n        hidden_layer_sizes=(100),\n        random_state=0,\n        max_iter=500,\n    ),\n)\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(f\"Accuracy: {clf.score(X_test, y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we use the CFI class to measure feature importance. Here, we use a RidgeCV\nmodel to estimate the conditional expectation $\\mathbb{E}[X^j | X^{-j}]$.\nSince this is a classification task, we use log_loss and specify the \"predict_proba\"\nmethod of our estimator.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import RidgeCV\nfrom sklearn.metrics import log_loss\n\nfrom hidimstat import CFI\n\ncfi = CFI(\n    estimator=clf,\n    loss=log_loss,\n    method=\"predict_proba\",\n    imputation_model_continuous=RidgeCV(),\n    features_groups={\n        feat_name: [i] for i, feat_name in enumerate(load_wine().feature_names)\n    },\n    random_state=0,\n)\ncfi.fit(\n    X_train,\n    y_train,\n)\nimportances = cfi.importance(X_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualization of CFI feature importance\nFinally, we visualize the importance of each feature using a bar plot.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n_, ax = plt.subplots(figsize=(6, 3))\nax = cfi.plot_importance(ax=ax)\nax.set_xlabel(\"Feature Importance\")\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Variable importance analysis is meant to help scientific understanding, in particular\nto identify which features are important to differentiate Barolo, Grignolino, and\nBarbera wine types.\nNote: Despite very large marginal importance, the features 'flavanoids' and '\ntotal_phenols' are not picked by CFI, probably due to their high correlation\n(0.86 between these two) and their redundancy with other features.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}