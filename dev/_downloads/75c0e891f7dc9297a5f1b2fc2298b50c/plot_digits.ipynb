{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Pixel-wise inference on digit classification\nThis example illustrates how to perform pixel-wise inference using Ensemble Clustered\nInference with Desparsified Lasso (EnCluDL) on digit classification tasks. We use the\nMNIST dataset and consider binary classification between digits 1 vs 7 and 0 vs 1. The\nMNIST dataset contains 28x28 pixel images of handwritten digits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the MNIST dataset\nWe start by loading the MNIST dataset from OpenML. We then filter the dataset to\ninclude only digits 4 and 7 for the first classification task, and digits 0 and 1\nfor the second task and digits 0 and 9 for the third task. To speed up the example, we\ndownsample the dataset to 4000 samples for each task.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.utils import resample\n\nmnist_dataset = fetch_openml(\"mnist_784\", version=1, as_frame=False)\nX_mnist, y_mnist = mnist_dataset.data, mnist_dataset.target\n# Downsample to speed up the example\nn_samples = 5000\nmask_4_7 = (y_mnist == \"4\") | (y_mnist == \"7\")\nX_4_7, y_4_7 = X_mnist[mask_4_7], y_mnist[mask_4_7].astype(int)\nX_4_7, y_4_7 = resample(\n    X_4_7, y_4_7, n_samples=n_samples, replace=False, random_state=0, stratify=y_4_7\n)\n\nmask_0_1 = (y_mnist == \"0\") | (y_mnist == \"1\")\nX_0_1, y_0_1 = X_mnist[mask_0_1], y_mnist[mask_0_1].astype(int)\nX_0_1, y_0_1 = resample(\n    X_0_1, y_0_1, n_samples=n_samples, replace=False, random_state=0, stratify=y_0_1\n)\n\nmask_0_9 = (y_mnist == \"0\") | (y_mnist == \"9\")\nX_0_9, y_0_9 = X_mnist[mask_0_9], y_mnist[mask_0_9].astype(int)\nX_0_9, y_0_9 = resample(\n    X_0_9, y_0_9, n_samples=n_samples, replace=False, random_state=0, stratify=y_0_9\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualizing samples from each classification task\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "_, axes = plt.subplots(3, 5, figsize=(6, 4), subplot_kw={\"xticks\": [], \"yticks\": []})\nfor i in range(5):\n    # Plot 0 vs 1\n    label = 1 if i % 2 == 0 else 0\n    axes[0, i].imshow(X_0_1[y_0_1 == label][i].reshape(28, 28), cmap=\"gray\")\n    # Plot 4 vs 7\n    label = 7 if i % 2 == 0 else 4\n    axes[1, i].imshow(X_4_7[y_4_7 == label][i].reshape(28, 28), cmap=\"gray\")\n    # PLot 0 vs 9\n    label = 9 if i % 2 == 0 else 0\n    axes[2, i].imshow(X_0_9[y_0_9 == label][i].reshape(28, 28), cmap=\"gray\")\n\naxes[0, 2].set_title(\"Digits 0 vs 1\", fontweight=\"bold\", y=1.0)\naxes[1, 2].set_title(\"Digits 4 vs 7\", fontweight=\"bold\", y=1.0)\naxes[2, 2].set_title(\"Digits 0 vs 9\", fontweight=\"bold\", y=1.0)\n\n_ = plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Randomness in clustering affects inference stability\nA key limitation of clustered inference procedures is that the clustering step\nintroduces randomness into the inference process. Data perturbations can produce\ndifferent clustering results, which subsequently lead to varying inference outcomes.\nThis effect can be demonstrated by running the clustering algorithm multiple times\non different subsamples of the data. Here we use the Ward hierarchical clustering\nalgorithm with spatial connectivity constraints (each pixel is connected to its\nimmediate neighbors) to cluster the pixels of the images. We use 100 clusters and\nvisualize the clustering results for four different subsamples of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import FeatureAgglomeration\nfrom sklearn.feature_extraction import image\n\nn_clusters = 100\nshape = (28, 28)\nconnectivity = image.grid_to_graph(n_x=shape[0], n_y=shape[1])\n\n\n_, axes = plt.subplots(1, 4, figsize=(9, 3))\n\nfor i, ax in enumerate(axes):\n    X_cluster = resample(X_4_7, n_samples=n_samples // 2, replace=False, random_state=i)\n    clustering = FeatureAgglomeration(n_clusters=n_clusters, connectivity=connectivity)\n    clustering.fit(X_cluster)\n    ax.imshow(clustering.labels_.reshape(28, 28), cmap=\"Set2\")\n    ax.axis(\"off\")\n    ax.set_title(f\"Clustering {i}\")\n\n_ = plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble Clustered Inference with Desparsified Lasso\nTo mitigate the randomness introduced by clustering, we ensemble the results from\nmultiple clustered inference procedures. This approach derandomizes the inference\nprocedure and produces more stable results. We use the class:`hidimstat.EnCluDL`\nfor this purpose with 10 bootstraps. While this approach is more computationally intensive than\nsingle clustered inference, the procedure can be parallelized across clustering\nrepetitions using the ``n_jobs`` parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import FeatureAgglomeration\nfrom sklearn.feature_extraction import image\nfrom sklearn.linear_model import LassoCV\n\nfrom hidimstat import DesparsifiedLasso\nfrom hidimstat.ensemble_clustered_inference import EnCluDL\n\nfwer = 0.1\nn_jobs = 5\nn_clusters = 100\n\nencludl = EnCluDL(\n    clustering=clustering,\n    desparsified_lasso=DesparsifiedLasso(estimator=LassoCV(max_iter=1000)),\n    n_bootstraps=10,\n    n_jobs=n_jobs,\n    random_state=0,\n    cluster_boostrap_size=0.5,\n)\n\nencludl.fit_importance(X_4_7, y_4_7)\nselected_pos_4_7 = encludl.pvalues_ < fwer / 2 / n_clusters\nselected_neg_4_7 = encludl.one_minus_pvalues_ < fwer / 2 / n_clusters\n\nencludl.fit_importance(X_0_1, y_0_1)\nselected_pos_0_1 = encludl.pvalues_ < fwer / 2 / n_clusters\nselected_neg_0_1 = encludl.one_minus_pvalues_ < fwer / 2 / n_clusters\n\nencludl.fit_importance(X_0_9, y_0_9)\nselected_pos_0_9 = encludl.pvalues_ < fwer / 2 / n_clusters\nselected_neg_0_9 = encludl.one_minus_pvalues_ < fwer / 2 / n_clusters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing the results\nFinally, we visualize the significant pixels identified by EnCluDL for each of the\nthree classification tasks.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\n_, axes = plt.subplots(1, 3, figsize=(5, 2), subplot_kw={\"xticks\": [], \"yticks\": []})\n\nfor i, (title, selected_pos, selected_neg) in enumerate(\n    [\n        (\"4 vs 7\", selected_pos_4_7, selected_neg_4_7),\n        (\"0 vs 1\", selected_pos_0_1, selected_neg_0_1),\n        (\"0 vs 9\", selected_pos_0_9, selected_neg_0_9),\n    ]\n):\n    mask_encludl = np.zeros(shape)\n    mask_encludl[selected_pos.reshape(shape)] = 1\n    mask_encludl[selected_neg.reshape(shape)] = -1\n\n    cmap = ListedColormap([\"tab:red\", \"white\", \"tab:blue\"])\n    axes[i].imshow(mask_encludl, cmap=cmap, vmin=-1, vmax=1)\n    axes[i].set_title(title, fontweight=\"bold\", y=1.0)\n\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing inference on handwritten digits is challenging because the images are not\nperfectly aligned, and the pixel regions occupied by digits vary from one image to\nanother. However, using EnCluDL, we can still identify clusters of pixels that are\nstatistically significant for the classification tasks. For example, we can detect\nthe bottom left portion of the loop when distinguishing between digits 0 and 9, the\nbottom left corner of digit 4 when distinguishing between digits 4 and 7, and the\ncentral vertical stroke when distinguishing between digits 0 and 1.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}