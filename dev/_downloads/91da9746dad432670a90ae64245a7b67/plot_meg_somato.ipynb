{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Source localization of somatosensory MEG data\n\nThis example adapts the experiment presented in\n:footcite:t:`chevalier2020statistical`. We show how to identify which cortical\nsources are activated during a somatosensory task. To do so, we leverage\nspatially constrained clustering to effectively reduce the dimensionality of\nthe problem while accounting for the spatial structure of the data. We then\nperform inference with the desparsified multitask Lasso to perform support\nrecovery from spatio-temporal data.\n\n.. admonition:: MultiTaskLasso:\n\n    The [MultiTaskLasso](https://scikit-learn.org/stable/modules/linear_model.html#multi-task-lasso)\n    is a linear model that estimates sparse coefficients for multiple correlated\n    tasks simultaneously. Here, the tasks correspond to the different time points.\n    Instead of fitting a separate Lasso model for each time point, it fits a single\n    model that enforces joint feature selection across all time points. To do so,\n    it uses an L1/L2 mixed-norm regularization, which for a coefficient matrix\n    $W$ of shape (n_features, n_tasks) is defined as:\n\n    .. math::\n        ||W||_{21} = \\sum_{j=1}^p \\sqrt{\\sum_{t=1}^T W_{j,t}^2}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load somatosensory MEG data from MNE-Python\nWe use the somatosensory MEG dataset available in MNE-Python. We visualize\nthe evoked response across MEG sensors.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import mne\nimport numpy as np\nfrom mne.datasets import somato\n\ncond = \"somato\"\ndata_path = somato.data_path(verbose=True)\nsubject = \"01\"\nsubjects_dir = data_path / \"derivatives/freesurfer/subjects\"\nraw_fname = (\n    data_path / f\"sub-{subject}\" / \"meg\" / f\"sub-{subject}_task-{cond}_meg.fif\"\n)\nfwd_fname = (\n    data_path\n    / \"derivatives\"\n    / f\"sub-{subject}\"\n    / f\"sub-{subject}_task-{cond}-fwd.fif\"\n)\n\n# Read evoked\nraw = mne.io.read_raw_fif(raw_fname)\nevents = mne.find_events(raw, stim_channel=\"STI 014\")\nreject = {\"grad\": 4000e-13, \"eog\": 350e-6}\npicks = mne.pick_types(raw.info, meg=True, eeg=True, eog=True)\n\nevent_id, tmin, tmax = 1, -0.2, 0.25\nepochs = mne.Epochs(\n    raw,\n    events,\n    event_id,\n    tmin,\n    tmax,\n    picks=picks,\n    reject=reject,\n    preload=True,\n)\nevoked = epochs.average()\nevoked = evoked.pick_types(\"grad\")\nevoked.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing MEG data for source localization\nBefore performing source localization, we need to preprocess the MEG data.\nTo do so we rely on the MNE-Python library. We compute the forward model,\nwhich describes how the activity of cortical sources is projected onto MEG\nsensors. We also select the time window of interest, which is the early\nresponse. Finally we compute the noise covariance matrix, in order to whiten\nthe data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from mne.inverse_sparse.mxne_inverse import _prepare_gain\n\n# Read forward matrix\nforward = mne.read_forward_solution(fwd_fname)\n# Compute noise covariance matrix\nnoise_cov = mne.compute_covariance(epochs, rank=\"info\", tmax=0.0)\n# We must reduce the whitener since data were preprocessed for removal\n# of environmental noise with maxwell filter leading to an effective\n# number of 64 samples.\npca = True\n# Preprocessing MEG data\nforward, gain, gain_info, whitener, _, _ = _prepare_gain(\n    forward,\n    evoked.info,\n    noise_cov,\n    pca=pca,\n    depth=0.0,\n    loose=0.0,\n    weights=None,\n    weights_min=None,\n    rank=None,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Selecting relevant time window: focusing on early signal\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t_min, t_max = 0.01, 0.05\nt_step = 1.0 / 300\n# Croping evoked according to relevant time window\nevoked.crop(tmin=t_min, tmax=t_max)\n# Choosing frequency and number of clusters used for compression.\n# Reducing the frequency to 100Hz to make inference faster\nstep = int(t_step * evoked.info[\"sfreq\"])\nevoked.decimate(step)\ny = np.dot(whitener, evoked.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Spatially constrained clustering\nWe then extract the spatial adjacency matrix that is then used in the\nclustering step to incorporate the spatial structure of the data.\nFor MEG data ``n_clusters = 750`` is generally a good default choice.\nTaking ``n_clusters > 2000`` might lead to an unpowerful inference.\nTaking ``n_clusters < 500`` might compress too much the data leading\nto a compressed problem not close enough to the original problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import FeatureAgglomeration\n\n# Collecting features' connectivity\nconnectivity = mne.source_estimate.spatial_src_adjacency(forward[\"src\"])\n\nn_clusters = 750\nward = FeatureAgglomeration(n_clusters=n_clusters, connectivity=connectivity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Running clustered inference\nWe can now run the clustered inference using the desparsified multitask Lasso\nWe then select the clusters that are significant at the target FWER level,\nwhich is set to 0.1 in this example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import MultiTaskLassoCV\n\nfrom hidimstat import CluDL, DesparsifiedLasso\n\n# Setting theoretical FWER target\nfwer_target = 0.1\n\ncludl = CluDL(\n    clustering=ward,\n    desparsified_lasso=DesparsifiedLasso(estimator=MultiTaskLassoCV()),\n    random_state=0,\n)\n\ncludl.fit_importance(gain, y)\nselected = cludl.fwer_selection(fwer_target, two_tailed_test=True)\n# multiplying the -log10(p-value) map by the sign of the estimated coefficients\nlog_pvalues = -np.log10(cludl.pvalues_) * selected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We here used CluDL, which relies on a single clustering. Alternatively,\n:class:`~hidimstat.EnCluDL` can be used, which relies on aggregating the\nresults over multiple clusterings obtained by running the clustering\nalgorithm on bootstrapped samples of the data. This can lead to more stable\nresults at the cost of a higher computational time.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the p-value map on the brain surface\nWe then visualize the identified cortical sources on the brain surface. We\nplot the -log10(p-value) map to which we assign the sign of the estimated\ncoefficient in the multitask Lasso model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nfrom nilearn import datasets, plotting\n\nstc_pvals = mne.SourceEstimate(\n    data=log_pvalues[:, np.newaxis],\n    vertices=[forward[\"src\"][0][\"vertno\"], forward[\"src\"][1][\"vertno\"]],\n    tmin=0.0,\n    tstep=1.0,\n    subject=f\"{subject}\",\n)\n\n# TODO: tmp fix: unlinking fsaverage directory to avoid error\nfs_dir = Path(subjects_dir) / \"fsaverage\"\nif fs_dir.is_symlink():\n    fs_dir.unlink()\nmne.datasets.fetch_fsaverage(subjects_dir=subjects_dir)\n\nmorph = mne.compute_source_morph(\n    src=forward[\"src\"],\n    subject_from=f\"{subject}\",\n    subject_to=\"fsaverage\",\n    subjects_dir=subjects_dir,\n    spacing=5,\n)\nstc_fsaverage = morph.apply(stc_pvals)\n\nstart_id = stc_fsaverage.data.shape[0] // 2\nrh_stat_map = stc_fsaverage.data[start_id:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fsaverage = datasets.fetch_surf_fsaverage(\"fsaverage5\")\n\nfig, axes = plt.subplots(\n    1,\n    2,\n    subplot_kw={\"projection\": \"3d\"},\n    figsize=(8, 5),\n    gridspec_kw={\"hspace\": -0.1},\n)\n\n# sphinx_gallery_thumbnail_number = 2\nfor ax, view in zip(axes, [\"lateral\", \"medial\"], strict=True):\n    plotting.plot_surf_stat_map(\n        surf_mesh=fsaverage.infl_right,\n        stat_map=rh_stat_map,\n        bg_map=fsaverage.sulc_right,\n        hemi=\"right\",\n        view=view,\n        colorbar=True,\n        cmap=\"RdBu_r\",\n        threshold=-np.log10(fwer_target),\n        vmax=10,\n        axes=ax,\n        title=f\"rh-{view} view\",\n    )\n\nax._colorbars[0].set_ylabel(\"-log10(p-value)\")\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most of the identified sources are located in the somatosensory cortex,\nwhich is expected for this somatosensory task.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}