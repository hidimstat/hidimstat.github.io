{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Variable Selection Under Model Misspecification\n\nIn this example, we illustrate the limitations of variable selection methods based on\nlinear models using the circles dataset. We first use the distilled conditional\nrandomization test (d0CRT), which is based on linear models :footcite:t:`liu2022fast` and then\ndemonstrate how model-agnostic methods, such as Leave-One-Covariate-Out (LOCO), can\nidentify important variables even when classes are not linearly separable.\n\nTo evaluate the importance of a variable, LOCO re-fits a sub-model using a subset of the\ndata where the variable of interest is removed. The importance of the variable is\nquantified as the difference in loss between the full model and the sub-model. As shown\nin :footcite:t:`williamson_2021_nonparametric` , this loss difference can be interpreted as an\nunnormalized generalized ANOVA (difference of R\u00b2).  Denoting $\\mu$ the predictive\nmodel used, $\\mu_{-j}$ the sub-model where the j-th variable is removed, and\n$X^{-j}$ the data with the j-th variable removed, the loss difference can be\nexpressed as:\n\n\\begin{align}\\psi_{j} = \\mathbb{V}(y) \\left[ \\left[ 1 - \\frac{\\mathbb{E}[(y - \\mu(X))^2]}{\\mathbb{V}(y)} \\right] - \\left[ 1 - \\frac{\\mathbb{E}[(y - \\mu_{-j}(X^{-j}))^2]}{\\mathbb{V}(y)} \\right] \\right]\\end{align}\n\nwhere $\\psi_{j}$ is the LOCO importance of the j-th variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate data where classes are not linearly separable\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.datasets import make_circles\n\nrng = np.random.default_rng(0)\nX, y = make_circles(\n    n_samples=500,\n    noise=0.1,\n    factor=0.6,\n    random_state=np.random.RandomState(rng.bit_generator),\n)\n\n\nfig, ax = plt.subplots()\nsns.scatterplot(\n    x=X[:, 0],\n    y=X[:, 1],\n    hue=y,\n    ax=ax,\n    palette=\"muted\",\n)\nax.legend(title=\"Class\")\nax.set_xlabel(\"X1\")\nax.set_ylabel(\"X2\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a linear and a non-linear estimator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import SVC\n\nnon_linear_model = SVC(kernel=\"rbf\", random_state=0)\nlinear_model = LogisticRegressionCV(\n    penalty=\"l1\",\n    solver=\"liblinear\",\n    max_iter=1000,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute p-values using d0CRT\nWe first compute the p-values using d0CRT which performs a conditional independence\ntest ($H_0: X_j \\perp\\!\\!\\!\\perp y | X_{-j}$) for each variable. However,\nthis test is based on a linear model (LogisticRegression) and fails to reject the null\nin the presence of non-linear relationships.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.base import clone\n\nfrom hidimstat import D0CRT\n\nd0crt_linear = D0CRT(\n    estimator=clone(linear_model), screening_threshold=None, random_state=0\n)\nd0crt_linear.fit_importance(X, y)\npval_dcrt_linear = d0crt_linear.pvalues_\nprint(f\"{pval_dcrt_linear=}\")\n\nd0crt_non_linear = D0CRT(\n    estimator=clone(non_linear_model), screening_threshold=None, random_state=0\n)\nd0crt_non_linear.fit_importance(X, y)\npval_dcrt_non_linear = d0crt_non_linear.pvalues_\nprint(f\"{pval_dcrt_non_linear=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute p-values using LOCO\nWe then compute the p-values using LOCO\nwith a linear, and then a non-linear model. When using a\nmisspecified model, such as a linear model for this dataset, LOCO fails to reject the null\nsimilarly to d0CRT. However, when using a non-linear model (SVC), LOCO is able to\nidentify the important variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import hinge_loss, log_loss\nfrom sklearn.model_selection import KFold\n\nfrom hidimstat import LOCO\n\ncv = KFold(n_splits=5, shuffle=True, random_state=0)\n\nimportances_linear = []\nimportances_non_linear = []\nfor train, test in cv.split(X):\n    non_linear_model_ = clone(non_linear_model)\n    linear_model_ = clone(linear_model)\n    non_linear_model_.fit(X[train], y[train])\n    linear_model_.fit(X[train], y[train])\n\n    vim_linear = LOCO(\n        estimator=linear_model_,\n        loss=log_loss,\n        method=\"predict_proba\",\n        n_jobs=2,\n    )\n    vim_non_linear = LOCO(\n        estimator=non_linear_model_,\n        loss=hinge_loss,\n        method=\"decision_function\",\n        n_jobs=2,\n    )\n    vim_linear.fit(X[train], y[train])\n    vim_non_linear.fit(X[train], y[train])\n\n    importances_linear.append(\n        vim_linear.importance(X[test], y[test])[\"importance\"],\n    )\n    importances_non_linear.append(\n        vim_non_linear.importance(X[test], y[test])[\"importance\"]\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To select variables using LOCO, we compute the p-values using a t-test over the\nimportance scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import ttest_1samp\n\n_, pval_linear = ttest_1samp(\n    importances_linear,\n    0,\n    axis=0,\n    alternative=\"greater\",\n)\n_, pval_non_linear = ttest_1samp(\n    importances_non_linear, 0, axis=0, alternative=\"greater\"\n)\n\nprint(f\"{pval_linear=}\")\nprint(f\"{pval_non_linear=}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the $-log_{10}(pval)$ for each method and variable\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndf_pval = pd.DataFrame(\n    {\n        \"pval\": np.hstack(\n            [\n                pval_dcrt_linear,\n                pval_dcrt_non_linear,\n                pval_linear,\n                pval_non_linear,\n            ]\n        ),\n        \"method\": [\"d0CRT-linear\"] * 2\n        + [\"d0CRT-non-linear\"] * 2\n        + [\"LOCO-linear\"] * 2\n        + [\"LOCO-non-linear\"] * 2,\n        \"Feature\": [\"X1\", \"X2\"] * 4,\n    }\n)\ndf_pval[\"minus_log10_pval\"] = -np.log10(df_pval[\"pval\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the $-log_{10}(pval)$ for each method and variable\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\nsns.barplot(\n    data=df_pval,\n    y=\"Feature\",\n    x=\"minus_log10_pval\",\n    hue=\"method\",\n    palette=\"muted\",\n    ax=ax,\n)\nax.set_xlabel(\"-$\\\\log_{10}(pval)$\")\nax.axvline(\n    -np.log10(0.05),\n    color=\"k\",\n    lw=3,\n    linestyle=\"--\",\n    label=\"-$\\\\log_{10}(0.05)$\",\n)\nax.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As expected, when using linear models (d0CRT and LOCO-linear) that are misspecified,\nthe variables are not selected. This highlights the benefit of using model-agnostic\nmethods such as LOCO, which allows for the use of models that are expressive enough\nto explain the data. While d0CRT can use any estimator, its distillation step\nrestricts it from capturing variable interactions.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}