{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Coefficient estimates with Desparsified Lasso on the diabetes dataset\n\nThis example illustrates how to compute de-biased coefficient estimates and confidence\nintervals using :class:`~hidimstat.DesparsifiedLasso` on the diabetes dataset.\nThis example is inspired by :footcite:t:`hastie2015statistical`.\n\nWhile the L1 penalty used in Lasso regression is a powerful regularization technique for\nbuilding predictive models, it introduces a bias in the coefficient estimates (shrinkage).\nWhen the goal is to interpret the importance of features or perform inference, this bias\nhas to be corrected. The Desparsified Lasso provides a method to obtain unbiased coefficient\nestimates, along with confidence intervals and p-values for hypothesis testing.\nRead more in the `User Guide <slm_methods>`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load diabetes dataset\nThe diabetes dataset is a well-known benchmark for regression tasks. It\ncontains 10 features corresponding to baseline measurements and a quantitative\nmeasure of disease progression.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n\ndata = load_diabetes(as_frame=True)\nX = data[\"data\"].to_numpy()\ny = data[\"target\"].to_numpy()\nfeature_names = data[\"data\"].columns.tolist()\ndata[\"data\"].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add spurious features\nTo evaluate the feature selection capabilities of the Desparsified Lasso, we\nartificially add spurious features. These are constructed as random linear\ncombinations of the original features plus noise, ensuring they are correlated\nwith the predictors but have no true association with the target variable.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\nX = StandardScaler().fit_transform(X)\ny = StandardScaler().fit_transform(y.reshape(-1, 1)).ravel()\n\nseed = 0\nrng = np.random.default_rng(seed=seed)\nn_spurious = 10\nX_spurious_list = []\nfor i in range(n_spurious):\n    X_spurious = (\n        X[:, rng.choice(X.shape[1], size=3, replace=False)]\n        + 1 * rng.normal(size=X[:, :3].shape)\n    ).sum(axis=1, keepdims=True)\n    X_spurious_normalized = StandardScaler().fit_transform(X_spurious)\n    X_spurious_list.append(X_spurious_normalized)\n    feature_names.append(f\"spurious_{i}\")\nX = np.hstack([X] + X_spurious_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Predictive performance benchmark\nBefore assessing feature importance, we evaluate the predictive performance of the\nLasso model (that will be used as base estimator in Desparsified Lasso) and a standard\nLinear Regression model using cross-validation. We expect the Lasso to perform\nbetter thanks to its regularization effect, especially with the added spurious features.\nWe visualize the correlation matrix of the features and the distribution of R2 scores.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LassoCV, LinearRegression\nfrom sklearn.model_selection import cross_val_score\n\nlasso_model = LassoCV(max_iter=1000)\nlinear_model = LinearRegression()\n\ncv_score_lasso = cross_val_score(lasso_model, X, y, cv=3)\ncv_score_linear = cross_val_score(linear_model, X, y, cv=3)\n\n_, ax = plt.subplots(1, 2, width_ratios=[2, 1], figsize=(7, 4))\ncorr_mat = data[\"data\"].corr()\nsns.heatmap(\n    corr_mat,\n    cmap=\"coolwarm\",\n    ax=ax[0],\n    cbar_kws={\"label\": \"Correlation\"},\n    mask=np.triu(np.ones_like(corr_mat, dtype=bool)),\n)\nsns.boxplot(data=[cv_score_lasso, cv_score_linear], ax=ax[1])\nax[1].set_xticklabels([\"Lasso\", \"Linear\\nRegression\"])\nax[1].set_ylabel(\"R2 score\")\nsns.despine()\nplt.tight_layout()\n_ = plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance with Desparsified Lasso\nWe fit the Desparsified Lasso on the dataset to obtain de-biased coefficient\nestimates and 95% confidence intervals.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\nfrom hidimstat import DesparsifiedLasso\n\ndl = DesparsifiedLasso(\n    estimator=LassoCV(max_iter=1000),\n    confidence=0.95,\n    model_x=LassoCV(),\n    n_jobs=5,\n    random_state=seed,\n)\ndl.fit_importance(X, y)\n\nselected = dl.fdr_selection(fdr=0.1, two_tailed_test=True)\ndf_plot = pd.DataFrame(\n    {\n        \"feature\": feature_names,\n        \"importance\": dl.importances_,\n        \"selected\": selected,\n        \"lasso_coef\": dl.estimator.coef_,\n        \"confidence_min\": dl.confidence_bound_min_,\n        \"confidence_max\": dl.confidence_bound_max_,\n    }\n)\ndf_plot.sort_values(by=\"importance\", key=np.abs, ascending=False, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results visualization\nWe visualize the de-biased coefficient estimates (circles) with their 95% confidence\nintervals and plot the original Lasso coefficient estimates (triangles) for comparison.\n\nWe observe that the confidence intervals help rule out spurious features that the\nstandard Lasso might otherwise select. For the non-spurious features, while the\nLasso coefficients are shrunk towards zero, the Desparsified Lasso provides a\ncorrection, often resulting in larger absolute coefficient estimates.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from matplotlib.lines import Line2D\n\n# sphinx_gallery_thumbnail_number = 2\n\n_, ax = plt.subplots(figsize=(6, 4))\n\nax.errorbar(\n    x=df_plot[\"feature\"],\n    y=df_plot[\"importance\"],\n    yerr=[\n        df_plot[\"importance\"] - df_plot[\"confidence_min\"],\n        df_plot[\"confidence_max\"] - df_plot[\"importance\"],\n    ],\n    ecolor=\"gray\",\n    capsize=8,\n    ls=\"\",\n)\nsns.pointplot(\n    data=df_plot,\n    x=\"feature\",\n    y=\"importance\",\n    hue=\"selected\",\n    linestyles=\"\",\n    palette=[\"tab:green\", \"tab:red\"],\n    markeredgewidth=0.5,\n    markeredgecolor=\"gray\",\n    markersize=8,\n)\nsns.pointplot(\n    data=df_plot,\n    x=\"feature\",\n    y=\"lasso_coef\",\n    hue=np.abs(df_plot[\"lasso_coef\"]) > 1e-3,\n    linestyles=\"\",\n    markers=\"^\",\n    palette=[\"tab:orange\", \"tab:blue\"],\n    markeredgewidth=0.5,\n    markeredgecolor=\"gray\",\n)\n\nlegend_elements = [\n    Line2D(\n        [0],\n        [0],\n        marker=m,\n        color=c,\n        label=label,\n        markersize=8,\n        linestyle=\"\",\n    )\n    for c, label, m in [\n        (\"tab:green\", \"Desparsified Lasso selected\", \"o\"),\n        (\"tab:red\", \"Desparsified Lasso not selected\", \"o\"),\n        (\"tab:blue\", \"Lasso coef $|\\\\beta| > 0$\", \"^\"),\n        (\"tab:orange\", \"Lasso coef $|\\\\beta| = 0$\", \"^\"),\n    ]\n]\n\nax.legend(handles=legend_elements, loc=\"best\")\n\nax.axhline(y=0, color=\"k\", linestyle=\"--\", linewidth=0.8)\nax.set_xticklabels(df_plot[\"feature\"], rotation=45, ha=\"right\")\nax.set_ylabel(\"$\\\\hat{\\\\beta}$: Coefficient estimates\")\nax.set_xlabel(\"\")\n\nsns.despine()\nplt.tight_layout()\n_ = plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "While some spurious features are selected by the Lasso, the Desparsified Lasso\nprovides better control over false discoveries. The combination of point estimates\nand confidence intervals allows for both robust feature selection and statistically-grounded feature\nimportance quantification.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# References\n# ----------\n# .. footbibliography::"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}