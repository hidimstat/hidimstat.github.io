
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "generated/gallery/examples/plot_knockoff_aggregation.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_generated_gallery_examples_plot_knockoff_aggregation.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_generated_gallery_examples_plot_knockoff_aggregation.py:


Knockoff aggregation on simulated data
======================================

In this example, we show an example of variable selection using
model-X Knockoffs introduced by :footcite:t:`candes2018panning`. A notable
drawback of this procedure is the randomness associated with generating
knockoff variables. This can result in fluctuations of the statistical power
and false discovery proportion, and consequently, unstable inference.

This example exhibits the two aggregation procedures described
by :footcite:t:`pmlr-v119-nguyen20a` and :footcite:t:`Ren_2023` to derandomize
inference.

.. GENERATED FROM PYTHON SOURCE LINES 15-31

.. code-block:: Python


    import matplotlib.pyplot as plt
    import numpy as np
    from joblib import Parallel, delayed
    from sklearn.linear_model import LassoCV
    from sklearn.model_selection import KFold

    from hidimstat._utils.scenario import multivariate_simulation
    from hidimstat.knockoffs import (
        model_x_knockoff,
        model_x_knockoff_bootstrap_e_value,
        model_x_knockoff_bootstrap_quantile,
        model_x_knockoff_pvalue,
    )
    from hidimstat.statistical_tools.multiple_testing import fdp_power








.. GENERATED FROM PYTHON SOURCE LINES 32-38

Data simulation
---------------
The comparison of the three methods relies on evaluating the
False Discovery Proportion (FDP) and statistical power. Assessing these
metrics requires knowledge of the actual data-generating process.
We therefore use simulated data with the following parameters:

.. GENERATED FROM PYTHON SOURCE LINES 38-64

.. code-block:: Python


    # number of repetitions of he methods
    runs = 20
    # Number of observations
    n_samples = 200
    # Number of variables
    n_features = 150
    # Correlation parameter
    rho = 0.5
    # Ratio of number of variables with non-zero coefficients over total
    # coefficients
    sparsity = 0.2
    # Desired controlled False Discovery Rate (FDR) level
    fdr = 0.1
    # signal noise ration
    signal_noise_ratio = 10
    # number of repetitions for the bootstraps
    n_bootstraps = 25
    # number of jobs for repetition of the method
    n_jobs = 2
    # verbosity of the joblib
    joblib_verbose = 0
    # Define the seeds for the reproducibility of the example
    rng = np.random.default_rng(42)









.. GENERATED FROM PYTHON SOURCE LINES 65-67

Define the function for running the three procedures on the same data
---------------------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 67-134

.. code-block:: Python

    def single_run(
        n_samples,
        n_features,
        rho,
        sparsity,
        signal_noise_ratio,
        fdr,
        n_bootstraps,
        seed=None,
    ):
        # Generate data
        X, y, beta_true, noise = multivariate_simulation(
            n_samples,
            n_features,
            rho=rho,
            support_size=int(n_features * sparsity),
            signal_noise_ratio=signal_noise_ratio,
            seed=seed,
        )
        non_zero_index = np.where(beta_true)[0]

        # Use model-X Knockoffs [1]
        selected, test_scores, threshold, X_tildes = model_x_knockoff(
            X,
            y,
            estimator=LassoCV(
                n_jobs=1,
                cv=KFold(n_splits=5, shuffle=True, random_state=0),
                random_state=1,
            ),
            n_bootstraps=1,
            random_state=2,
        )
        mx_selection, _ = model_x_knockoff_pvalue(test_scores, fdr=fdr)
        fdp_mx, power_mx = fdp_power(mx_selection, non_zero_index)

        # Use aggregation model-X Knockoffs [2]
        selected, test_scores, threshold, X_tildes = model_x_knockoff(
            X,
            y,
            estimator=LassoCV(
                n_jobs=1,
                cv=KFold(n_splits=5, shuffle=True, random_state=3),
                random_state=4,
            ),
            n_bootstraps=n_bootstraps,
            n_jobs=1,
            random_state=5,
        )

        # Use p-values aggregation [2]
        aggregated_ko_selection, _, _ = model_x_knockoff_bootstrap_quantile(
            test_scores, fdr=fdr, adaptive_aggregation=True
        )

        fdp_pval, power_pval = fdp_power(aggregated_ko_selection, non_zero_index)

        # Use e-values aggregation [3]
        eval_selection, _, _ = model_x_knockoff_bootstrap_e_value(
            test_scores, threshold, fdr=fdr
        )

        fdp_eval, power_eval = fdp_power(eval_selection, non_zero_index)

        return fdp_mx, fdp_pval, fdp_eval, power_mx, power_pval, power_eval









.. GENERATED FROM PYTHON SOURCE LINES 135-137

Define the function for plotting the result
-------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 137-169

.. code-block:: Python

    def plot_results(bounds, fdr, n_samples, n_features, power=False):
        plt.figure(figsize=(5, 5), layout="constrained")
        for nb in range(len(bounds)):
            for i in range(len(bounds[nb])):
                y = bounds[nb][i]
                x = rng.normal(nb + 1, 0.05)
                plt.scatter(x, y, alpha=0.65, c="blue")

        plt.boxplot(bounds, sym="")
        if power:
            plt.xticks(
                [1, 2, 3],
                ["MX Knockoffs", "Quantile aggregation", "e-values aggregation"],
                rotation=45,
                ha="right",
            )
            plt.title(f"FDR = {fdr}, n = {n_samples}, p = {n_features}")
            plt.ylabel("Empirical Power")

        else:
            plt.hlines(fdr, xmin=0.5, xmax=3.5, label="Requested FDR control", color="red")
            plt.xticks(
                [1, 2, 3],
                ["MX Knockoffs", "Quantile aggregation", "e-values aggregation"],
                rotation=45,
                ha="right",
            )
            plt.title(f"FDR = {fdr}, n = {n_samples}, p = {n_features}")
            plt.ylabel("Empirical FDP")
            plt.legend(loc="best")









.. GENERATED FROM PYTHON SOURCE LINES 170-172

Define the function for evaluate the effect of the population
-------------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 172-213

.. code-block:: Python

    def effect_number_samples(n_samples):
        parallel = Parallel(n_jobs, verbose=joblib_verbose)
        results = parallel(
            delayed(single_run)(
                n_samples,
                n_features,
                rho,
                sparsity,
                signal_noise_ratio,
                fdr,
                n_bootstraps,
                seed=seed,
            )
            for seed in range(runs)
        )

        fdps_mx = []
        fdps_pval = []
        fdps_eval = []
        powers_mx = []
        powers_pval = []
        powers_eval = []
        for fdp_mx, fdp_pval, fdp_eval, power_mx, power_pval, power_eval in results:
            fdps_mx.append(fdp_mx)
            fdps_pval.append(fdp_pval)
            fdps_eval.append(fdp_eval)

            powers_mx.append(power_mx)
            powers_pval.append(power_pval)
            powers_eval.append(power_eval)

        # Plot FDP and Power distributions

        fdps = [fdps_mx, fdps_pval, fdps_eval]
        powers = [powers_mx, powers_pval, powers_eval]

        plot_results(fdps, fdr, n_samples, n_features)
        plot_results(powers, fdr, n_samples, n_features, power=True)
        plt.show()









.. GENERATED FROM PYTHON SOURCE LINES 214-216

Aggregation methods provide a more stable inference
---------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 216-218

.. code-block:: Python

    effect_number_samples(n_samples=n_samples)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_001.png
         :alt: FDR = 0.1, n = 200, p = 150
         :srcset: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_001.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_002.png
         :alt: FDR = 0.1, n = 200, p = 150
         :srcset: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_002.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.560e+01, tolerance: 7.027e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.562e+01, tolerance: 7.841e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.030e+00, tolerance: 7.985e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.975e+00, tolerance: 6.992e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.626e+00, tolerance: 7.474e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.974e+01, tolerance: 8.503e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.784e+01, tolerance: 8.269e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.034e+01, tolerance: 7.897e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.468e+01, tolerance: 9.686e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.881e+01, tolerance: 9.932e-01
      model = cd_fast.enet_coordinate_descent(




.. GENERATED FROM PYTHON SOURCE LINES 219-223

By repeating the model-X Knockoffs, we can see that instability
of the inference. Additionally, we can see that the aggregation method
is more stable. However, the e-values aggregation is more conservative,
i.e. the expect variables of importance is not find.

.. GENERATED FROM PYTHON SOURCE LINES 225-227

Limitation of the aggregation methods
-------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 227-229

.. code-block:: Python

    effect_number_samples(n_samples=50)




.. rst-class:: sphx-glr-horizontal


    *

      .. image-sg:: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_003.png
         :alt: FDR = 0.1, n = 50, p = 150
         :srcset: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_003.png
         :class: sphx-glr-multi-img

    *

      .. image-sg:: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_004.png
         :alt: FDR = 0.1, n = 50, p = 150
         :srcset: /generated/gallery/examples/images/sphx_glr_plot_knockoff_aggregation_004.png
         :class: sphx-glr-multi-img


.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.070e-01, tolerance: 1.637e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.888e-01, tolerance: 1.637e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.456e-01, tolerance: 2.001e-01
      model = cd_fast.enet_coordinate_descent(
    /home/circleci/project/.venv/lib/python3.13/site-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.725e-01, tolerance: 2.001e-01
      model = cd_fast.enet_coordinate_descent(




.. GENERATED FROM PYTHON SOURCE LINES 230-232

One important point of this method is that they require enough samples to
estimate the distribution of each feature.

.. GENERATED FROM PYTHON SOURCE LINES 234-237

References
----------
.. footbibliography::


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 54.802 seconds)


.. _sphx_glr_download_generated_gallery_examples_plot_knockoff_aggregation.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_knockoff_aggregation.ipynb <plot_knockoff_aggregation.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_knockoff_aggregation.py <plot_knockoff_aggregation.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: plot_knockoff_aggregation.zip <plot_knockoff_aggregation.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
