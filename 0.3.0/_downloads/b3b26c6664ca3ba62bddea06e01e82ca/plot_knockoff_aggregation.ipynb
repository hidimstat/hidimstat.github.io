{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Knockoff aggregation\n\nThe examples shows how to use aggregate model-X Knockoff selections to derandomize\ninference. The model-X Knockoff introduced by :footcite:t:`candes2018panning` allows\nfor variable selection with statistical guarantees on the False Discovery Rate (FDR),\n\n\\begin{align}FDR = \\mathbb{E}[ FDP ] = \\mathbb{E} \\left[ \\frac{|\\hat{S} \\cap \\mathcal{H}_0 | }{| \\hat{S} |} \\right]\\end{align}\n\nwhere $\\hat{S}$ is the set of selected variables and $\\mathcal{H}_0$ is the\nset of null variables (i.e., variables with no effect on the response).\nA notable drawback of this procedure is the randomness associated with generating\nknockoff variables, $\\tilde{X}$. This can result in fluctuations of the statistical\npower and false discovery proportion, and consequently, unstable inference.\n\nTo mitigate this issue, several aggregation procedures have been proposed in the\nliterature. :footcite:t:`pmlr-v119-nguyen20a` introduces a quantile aggregation\nprocedure based on the p-values obtained from multiple independent runs of the\nknockoff filter. Or :footcite:t:`Ren_2023` proposes an aggregation procedure based on\ne-values. We illustrate both procedures in this example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating data\nWe use a simulated dataset where we know the ground truth to evaluate the\nperformance, in terms of statistical power and false discovery proportion, of the\ndifferent aggregation procedures. We generate data with `n=300` samples and `p=100`\ncorrelated features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom hidimstat._utils.scenario import multivariate_simulation\n\nn_features = 100\nn_samples = 300\n# Correlation\nrho = 0.5\n# Sparsity of the support\nsparsity = 0.5\n# Signal-to-noise ratio\nsnr = 10\n\n# Generate data\nX, y, beta_true, noise = multivariate_simulation(\n    n_samples=n_samples,\n    n_features=n_features,\n    rho=rho,\n    support_size=int(n_features * sparsity),\n    signal_noise_ratio=snr,\n    seed=0,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference with model-X Knockoffs\nWe repeat the model-X Knockoff procedure multiple times, as controlled by the\n`n_repeats` parameter, to obtain different selections. This will allow us to\nobserve the variability of the selections induced by the knockoff lottery. Then, we\ncompare the possible solutions to aggregate the selections in order to derandomize\nthe inference.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat.knockoffs import ModelXKnockoff\nfrom hidimstat.statistical_tools.multiple_testing import fdp_power\n\nfdr = 0.1\nn_repeats = 25\nn_jobs = 4\nmodel_x_knockoff = ModelXKnockoff(n_repeats=n_repeats, n_jobs=n_jobs, random_state=0)\nmodel_x_knockoff.fit_importance(X, y)\n\nfdp_individual = []\npower_individual = []\nmodel_x_knockoff.importances_.shape\nfor ko_statistics in model_x_knockoff.importances_:\n    threshold = model_x_knockoff.knockoff_threshold(ko_statistics, fdr=fdr)\n    ko_selection = ko_statistics > threshold\n\n    fdp, power = fdp_power(ko_selection, ground_truth=beta_true)\n    fdp_individual.append(fdp)\n    power_individual.append(power)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the results of the individual selections\nWe first visualize the results of the individual selections to observe the\nvariability induced by the knockoff lottery. We plot the False Discovery Proportion\n(FDP) for each run along with the desired FDR level (red dashed line) and the statistical\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndf_plot = pd.DataFrame(\n    {\n        \"FDP\": fdp_individual,\n        \"Power\": power_individual,\n    }\n)\n\n_, axes = plt.subplots(1, 2, figsize=(5, 3.5))\n\nax = axes[0]\nsns.swarmplot(\n    data=df_plot,\n    y=\"FDP\",\n    ax=ax,\n)\nax.axhline(fdr, color=\"tab:red\", linestyle=\"--\", lw=2, label=\"Desired FDR\")\nax.scatter(\n    0,\n    np.mean(fdp_individual),\n    marker=\"d\",\n    color=\"tab:orange\",\n    s=100,\n    zorder=10,\n    label=\"Empirical FDR\",\n)\nax.legend(framealpha=0.2)\n# Plot the power\nax = axes[1]\nsns.swarmplot(\n    data=df_plot,\n    y=\"Power\",\n    ax=ax,\n)\nsns.despine()\n_ = plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aggregation procedures\nWe now compute the aggregation using both the p-values aggregation procedure from\n:footcite:t:`pmlr-v119-nguyen20a` and the e-values aggregation procedure from\n:footcite:t:`Ren_2023`. We then compare the results of both procedures in terms of\nFDP and power.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pval_aggregation = model_x_knockoff.fdr_selection(fdr=fdr, adaptive_aggregation=True)\nfdp_pval_agg, power_pval_agg = fdp_power(pval_aggregation, ground_truth=beta_true)\n\neval_aggregation = model_x_knockoff.fdr_selection(\n    fdr=fdr, fdr_control=\"ebh\", evalues=True\n)\nfdp_eval_agg, power_eval_agg = fdp_power(eval_aggregation, ground_truth=beta_true)\n\ndf_plot[\"Method\"] = \"Individual KO\"\ndf_plot_2 = pd.concat(\n    [\n        df_plot,\n        pd.DataFrame(\n            {\n                \"FDP\": [fdp_pval_agg],\n                \"Power\": [power_pval_agg],\n                \"Method\": [\"P-value aggregation\"],\n            }\n        ),\n        pd.DataFrame(\n            {\n                \"FDP\": [fdp_eval_agg],\n                \"Power\": [power_eval_agg],\n                \"Method\": [\"E-value aggregation\"],\n            }\n        ),\n    ],\n    ignore_index=True,\n)\n\n# Plot the results\n# ----------------\n# In addition to the individual selections (blue), we plot the FDR and power obtained by\n# p-value aggregation (orange) and e-value aggregation (green).\n\n# sphinx_gallery_thumbnail_number = 2\n_, axes = plt.subplots(1, 2, figsize=(6, 3.5))\nax = axes[0]\nsns.stripplot(\n    data=df_plot_2,\n    y=\"FDP\",\n    hue=\"Method\",\n    ax=ax,\n    palette=\"muted\",\n    dodge=1,\n    legend=False,\n    size=8,\n    linewidth=1,\n)\nax.axhline(fdr, color=\"tab:red\", linestyle=\"--\", lw=2, label=\"Desired FDR\")\n\nax = axes[1]\nsns.stripplot(\n    data=df_plot_2,\n    y=\"Power\",\n    hue=\"Method\",\n    ax=ax,\n    palette=\"muted\",\n    dodge=True,\n    size=8,\n    linewidth=1,\n)\nsns.despine()\n_ = plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It appears that both aggregation procedures successfully lowers the false discovery\nproportion while maintaining a good statistical power.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}