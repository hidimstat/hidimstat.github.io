{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Feature Importance on diabetes dataset using cross-validation\n\nIn this example, we show how to compute variable importance using Permutation Feature\nImportance (PFI), Leave-One-Covariate-Out (LOCO), and Conditional Feature Importance\n(CFI) on the diabetes dataset. This example also showcases the use how to measure\nfeature importance in a K-Fold cross-validation setting in order to use all the data\navailable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the diabetes dataset\nWe start by loading the diabetes dataset from sklearn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_diabetes\n\ndiabetes = load_diabetes()\nX, y = diabetes.data, diabetes.target\n\n# Encode sex as binary\nX[:, 1] = (X[:, 1] > 0.0).astype(int)\nprint(f\"Number of samples: {X.shape[0]}, number of features: {X.shape[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a baseline model on the diabetes dataset\nThe benefit of perturbation-based variable importance methods, presented in this\nexample, is that they are model-agnostic. Therefore, we can use any regression\nmodel. We here leverage this flexibility, using an ensemble model which consists of a\nRidge regression model and a Histogram Gradient Boosting model, a Random Forest model,\nand a Lasso regression model combined with a Voting Regressor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom sklearn.base import clone\nfrom sklearn.ensemble import (\n    HistGradientBoostingRegressor,\n    RandomForestRegressor,\n    VotingRegressor,\n)\nfrom sklearn.linear_model import LassoCV, RidgeCV\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import KFold\n\nn_folds = 5\ncv = KFold(n_splits=n_folds, shuffle=True, random_state=0)\nregressor = VotingRegressor(\n    [\n        (\"ridge\", RidgeCV()),\n        (\"hgb\", HistGradientBoostingRegressor()),\n        (\"rf\", RandomForestRegressor()),\n        (\"lasso\", LassoCV()),\n    ]\n)\n\nscores = []\nregressor_list = [clone(regressor) for _ in range(n_folds)]\nfor i, (train_index, test_index) in enumerate(cv.split(X)):\n    regressor_list[i].fit(X[train_index], y[train_index])\n    scores.append(\n        r2_score(y_true=y[test_index], y_pred=regressor_list[i].predict(X[test_index]))\n    )\nprint(f\"R2 scores across folds: {np.mean(scores):.3f} \u00b1 {np.std(scores):.3f}\")\nregressor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measure the importance of variables\nWe now measure the importance of each variable using the three different methods:\nConditional Feature Importance (CFI), Leave-One-Covariate-Out (LOCO), and\nPermutation Feature Importance (PFI). We use the K-Fold cross-validation scheme to\nleverage all the data available. This however comes with the challenge that the\ntest statistics computed across folds are not independent since overlapping training\nsets are used to fit the model. To address this issue, we use the Nadeau-Bengio\ncorrected t-test :footcite:t:`nadeau1999inference` which adjusts the variance\nestimation to account for the dependency between the test statistics. We use the\n`n_jobs` parameter to parallelize the computation across folds.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import CFICV\n\ncfi_cv = CFICV(\n    estimators=regressor_list,\n    cv=cv,\n    n_jobs=5,\n    statistical_test=\"nb-ttest\",\n    random_state=0,\n)\nimportances_cfi = cfi_cv.fit_importance(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We repeat the same process using the LOCO method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import LOCOCV\n\nloco_cv = LOCOCV(\n    estimators=regressor_list,\n    cv=cv,\n    n_jobs=5,\n    statistical_test=\"nb-ttest\",\n)\nimportances_loco = loco_cv.fit_importance(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we repeat the same process using the PFI method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from hidimstat import PFICV\n\npfi_cv = PFICV(\n    estimators=regressor_list,\n    cv=cv,\n    n_jobs=5,\n    statistical_test=\"nb-ttest\",\n    random_state=0,\n)\nimportances_pfi = pfi_cv.fit_importance(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyze the results\nFinally, we visualize the results obtained with the three different methods. We plot\nthe negative log10 p-values for each variable and each method. A horizontal red-dashed\nline indicates the significance threshold at p-value=0.05.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ndf_plot = pd.concat(\n    [\n        pd.DataFrame(\n            {\n                \"var\": diabetes.feature_names,\n                \"importance\": vim.importances_.mean(axis=1),\n                \"pval\": vim.pvalues_,\n                \"method\": vim_name,\n            }\n        )\n        for vim, vim_name in zip(\n            [cfi_cv, loco_cv, pfi_cv],\n            [\"CFI\", \"LOCO\", \"PFI\"],\n        )\n    ]\n)\ndf_plot = df_plot.sort_values(\"importance\", ascending=False)\ndf_plot[\"log10pval\"] = -np.log10(df_plot[\"pval\"])\n\n_, ax = plt.subplots()\nsns.barplot(\n    data=df_plot,\n    x=\"var\",\n    y=\"log10pval\",\n    hue=\"method\",\n    ax=ax,\n)\nax.axhline(-np.log10(0.05), color=\"tab:red\", ls=\"--\", label=\"pval=0.05\")\nax.set_ylabel(r\"$-\\log_{10}(\\text{p-value})$\")\nax.legend(title=\"Method\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Several trends can be observed from the results: PFI tends to give smaller p-values\n(that is higher bars in the plot) than LOCO and CFI. This is expected since PFI is\nknown to overestimate the importance of correlated variables. On the other hand, LOCO\nhas in general, larger p-values (smaller bars in the plot). This is also a known trend\nsince LOCO tends to suffer from lower statistical power.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n.. footbibliography::\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}